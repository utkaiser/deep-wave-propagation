{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This notebook explains how to train the end-to-end model of the master's thesis “Fast, Accurate, and Scalable Numerical Wave Propagation: Enhancement by Deep Learning” by Luis Kaiser, supervised by Prof. Tsai (University of Texas Austin) and Prof. Klingenberg (University of Wuerzburg), in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "First, make sure that you have install all necessary libraries specified in `requirements.txt` using `pip` or `pip3` depending on your setup by running the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip3 install --upgrade pip\n",
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We made this notebook easier to read with less complex dependencies in the code compared to the original implementation. We first load the data, then set up the model with optimizer and lastly train and evaluate the model. We use `datagen_test.npz` as our train and test data. Please note that the model does not train well for small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import save_model, Model_end_to_end\n",
    "from config import get_params\n",
    "from generate_data import fetch_data_end_to_end\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model_name = \"test\",\n",
    "    lr = .001,\n",
    "    batch_size = 1,\n",
    "    n_epochs = 10,\n",
    "    downsampling_model = \"Interpolation\",\n",
    "    upsampling_model = \"UNet3\",\n",
    "    data_paths = \"data/datagen_test.npz\",\n",
    "    val_paths = \"data/datagen_test2.npz\"\n",
    "):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : (string) name of model, used as name for output-file containing model parameters\n",
    "    lr : (float) learning rate of model\n",
    "    batch_size : (int) batch size\n",
    "    n_epochs : (int) number of iterations model sees all data; i.e. amount of epochs model is trained\n",
    "    downsampling_model : (string) name of downsampling model\n",
    "    upsampling_model : (string) name of upsampling model\n",
    "    data_paths : (string)\n",
    "    val_paths : (string)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    trained model parameters in \".pt\"-file\n",
    "    '''\n",
    "\n",
    "    # model setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    param_dict = get_params()  # get dict of params contains all model specifications such as numerical params\n",
    "    model = Model_end_to_end(param_dict, downsampling_model, upsampling_model)\n",
    "    model = torch.nn.DataParallel(model).to(device)  # multi-GPU use\n",
    "\n",
    "    # data setup\n",
    "    train_loader, val_loader, _ = fetch_data_end_to_end([data_paths], batch_size, [val_paths])\n",
    "\n",
    "    # deep learning setup\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)  # initialize optimizer\n",
    "    loss_f = torch.nn.MSELoss()  # initialize loss function\n",
    "\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # training\n",
    "        model.train()\n",
    "\n",
    "        train_loss_list = []  # initialize list to store loss values of training\n",
    "\n",
    "        for i, data in enumerate(train_loader):  # iterate over datapoints in train_loader\n",
    "            loss_list = []  # create tmp loss list for backpropagating multiple losses at once\n",
    "            n_snaps = data[0].shape[1]  # number of snapshots defined by data input\n",
    "            data = data[0].to(device)  # use GPUs if available for faster training\n",
    "\n",
    "            # choose n_snaps start indices, possible points are in [0, ..., n_snaps - 2] and chosen randomly\n",
    "            # i.e. (input_idx * dt_star) point in time is when we start using the data\n",
    "            for input_idx in random.choices(range(n_snaps - 2), k=n_snaps):\n",
    "                input_tensor = data[:, input_idx].detach()  # detach because if not computation graph would go too far back\n",
    "\n",
    "                # one-step loss function (this loop is used to show that we can use something else for\n",
    "                # \"input_idx + 2\" to get a multi-step-loss approach (cf. paper)\n",
    "                for label_idx in range(input_idx + 1, input_idx + 2):\n",
    "                    label = data[:, label_idx, :3]\n",
    "                    output = model(input_tensor, data[:,input_idx, 4].detach())  # apply end-to-end model\n",
    "                    loss_list.append(loss_f(output, label))  # save loss\n",
    "\n",
    "                    # IMPORTANT: save current result to use for next iteration if multi-step-loss used\n",
    "                    input_tensor = torch.cat((output, input_tensor[:, 3].unsqueeze(dim=1)), dim=1)\n",
    "\n",
    "            # optimizer stepping\n",
    "            optimizer.zero_grad()\n",
    "            sum(loss_list).backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # save loss to later print out\n",
    "            train_loss_list.append(np.array([l.cpu().detach().numpy() for l in loss_list]).mean())\n",
    "\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # initialize list to save losses\n",
    "            val_loss_list = []\n",
    "\n",
    "            for i, data in enumerate(val_loader):  # iterate over datapoints in val_loader\n",
    "                n_snaps = data[0].shape[1]  # number of snapshots defined by data input\n",
    "                data = data[0].to(device)  # use GPUs if available for faster execution\n",
    "\n",
    "                input_tensor = data[:, 0].detach()  # detach because if not computation graph would go too far back\n",
    "                vel = input_tensor[:, 3].unsqueeze(dim=1)  # access velocity profile in input_tensor\n",
    "\n",
    "                for label_idx in range(1, n_snaps):  # advance a wave field for (n_snaps - 1) time steps\n",
    "                    label = data[:, label_idx, :3]\n",
    "                    output = model(input_tensor, data[:, 0, 4].detach())  # apply end-to-end model\n",
    "\n",
    "                    # get and save loss (this could be optimized by using a metric)\n",
    "                    val_loss_list.append(loss_f(output, label).item())\n",
    "\n",
    "                    # IMPORTANT: save current result to use for next iteration if multi-step-loss used\n",
    "                    input_tensor = torch.cat((output, vel), dim=1)\n",
    "\n",
    "        print(f'epoch %d, train loss: %.5f, test loss: %.5f'\n",
    "              %(epoch + 1, np.array(train_loss_list).mean(), np.array(val_loss_list).mean()))\n",
    "\n",
    "    # save model parameters as a \".pt\"-file, use empty string for directory path\n",
    "    save_model(model, model_name, \"results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
